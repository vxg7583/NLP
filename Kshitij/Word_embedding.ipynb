{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk.tokenize as nt\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB movies review dataset \n",
    "df = pd.read_csv(\"IMDBDataset.csv\")\n",
    "df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Tokenize and creating word index....\nDone\nFound 5021 words in the dataset\n"
    }
   ],
   "source": [
    "# Extract movies reviews from the dataset in an array\n",
    "sentences = df[\"review\"].fillna(\"DUMMY_VALUES\").values\n",
    "filtered_sentence = []\n",
    "word_index = {}\n",
    "lemma_function = WordNetLemmatizer()\n",
    "idx = 1\n",
    "print(\"Tokenize and creating word index....\")\n",
    "for sentence in sentences:\n",
    "    # Removing useless charecters from the string \n",
    "    s = sentence.replace('<br />', '')\n",
    "    filtered_sentence.append(s)\n",
    "    \n",
    "    # Tokenize every sentence in dataset\n",
    "    tok_sentence = nt.word_tokenize(s.lower())\n",
    "    \n",
    "    # Lemmatizing the words in every sentence and creaing a word index dictionay\n",
    "    # This Dictionary will maps numeric index values to words which can pe used later in the model \n",
    "    for word in tok_sentence:\n",
    "        #word = lemma_function.lemmatize(word)\n",
    "        if word not in word_index:\n",
    "            word_index[word]=idx\n",
    "            idx+=1\n",
    "print(\"Done\")\n",
    "print(\"Found %d words in the dataset\" %len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    r_l = []\n",
    "    for i in l:\n",
    "        if len(i)>0:\n",
    "            temp = i.replace(\"<br />\", \"\")\n",
    "            r_l.append(temp.lower())\n",
    "    return r_l\n",
    "\n",
    "def count(sentences):\n",
    "    f_sentences = flatten(sentences)\n",
    "    counts = Counter()\n",
    "    for i in f_sentences:\n",
    "        counts.update(re.findall('\\w+',i))\n",
    "    counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    c = {}\n",
    "    for i in counts:\n",
    "        c[i[0]]=i[1]\n",
    "    return c,counts\n",
    "count_dict, count_set = count(sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2 \n",
    "EMB_DIM = 50\n",
    "TABLE_SIZE = 1e8\n",
    "alpha = 0.75\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([0., 1., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([0., 0., 1.,  ..., 0., 0., 0.])),\n",
       " (tensor([0., 1., 0.,  ..., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       " (tensor([0., 0., 1.,  ..., 0., 0., 0.]),\n",
       "  tensor([0., 1., 0.,  ..., 0., 0., 0.])),\n",
       " (tensor([0., 0., 1.,  ..., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       " (tensor([0., 0., 1.,  ..., 0., 0., 0.]),\n",
       "  tensor([0., 0., 0.,  ..., 0., 0., 0.]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_target_pair = []\n",
    "\n",
    "for sentence in filtered_sentence:\n",
    "    tok_filtered_sentence = nt.word_tokenize(sentence.lower())\n",
    "    for i, word in enumerate(tok_filtered_sentence):\n",
    "        #print(word)\n",
    "        for cont_ran in range(-CONTEXT_SIZE,CONTEXT_SIZE+1):\n",
    "            try:\n",
    "                if cont_ran!=0 and i+cont_ran>=0:\n",
    "                    i_word = index_lookup(word)\n",
    "                    i_neri = index_lookup( tok_filtered_sentence[i+cont_ran])\n",
    "                    temp_w = to_categorical(i_word,N)\n",
    "                    temp_n = to_categorical(i_neri,N)\n",
    "                    temp = (torch.from_numpy(np.asarray(temp_w)), torch.from_numpy(np.asarray(temp_n)))\n",
    "                    #print(temp)\n",
    "                    input_target_pair.append(temp)   \n",
    "            except IndexError:\n",
    "                continue  \n",
    "    \n",
    "    \n",
    "input_target_pair[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total number of words:  23490\nNumber of words removed:  2457\nserial\nprevious\nsex\nseeking\nfloating\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['serial', 'previous', 'sex', 'seeking', 'floating']"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "def prob_frequency(count, t = 1e-5):\n",
    "    prob = {}\n",
    "    N = sum(x[1] for x in count)\n",
    "    print (\"Total number of words: \",N)\n",
    "    for k,v in count:\n",
    "        p = v/N\n",
    "        val = np.sqrt(t * N /v)\n",
    "        pk = val * (1 + val)\n",
    "        #pk = (np.sqrt(p/t) + 1) * (t/p)\n",
    "        prob[k] = pk\n",
    "    return prob\n",
    "\n",
    "def subsampling(probs,filtered_sentence):\n",
    "    train = []\n",
    "    t= set()\n",
    "    for sentence in filtered_sentence:\n",
    "        tok_filtered_sentence = nt.word_tokenize(sentence.lower())\n",
    "        temp = \"\"\n",
    "        for word in tok_filtered_sentence:\n",
    "            try:\n",
    "                if np.random.random() < (probs[word]):\n",
    "                    temp+=word + \" \"\n",
    "                else:\n",
    "                    t.add(word) \n",
    "            except KeyError:\n",
    "                continue\n",
    "        train.append(temp)\n",
    "    print(\"Number of words removed: \",len(t))\n",
    "    return train\n",
    "\n",
    "def negative_sampling(train,alpha,TABLE_SIZE,K):\n",
    "    l = []\n",
    "    for i in train:\n",
    "        tok = nt.word_tokenize(i)\n",
    "        bigrams = ngrams(tok,1)\n",
    "        for b in bigrams:\n",
    "            l.append(b)\n",
    "    c = Counter(l)\n",
    "    TABLE_SIZE = int(TABLE_SIZE)\n",
    "    s = sum(v for k,v in c.items())\n",
    "    p_sum = sum((v/s)**alpha for k,v in c.items())\n",
    "    neg_dict = {}\n",
    "    for k,v in c.items():\n",
    "        p = ((v/s)**alpha)/p_sum\n",
    "        neg_dict[k[0]]=p\n",
    "    table_count = []\n",
    "    for k,v in neg_dict.items():\n",
    "        count = np.round(v*TABLE_SIZE)\n",
    "        table_count.append((k,count))\n",
    "    idx = 0 \n",
    "    inc= 0\n",
    "    unigram_table = []\n",
    "    N=len(table_count)\n",
    "    for a in range(TABLE_SIZE):\n",
    "        unigram_table.append(table_count[idx][0])\n",
    "        if(inc == int(table_count[idx][1]) and idx < N):\n",
    "            inc = 0\n",
    "            idx+=1\n",
    "        if(idx==N):\n",
    "            break\n",
    "        inc+=1\n",
    "       # pass\n",
    "    rand = random.choices(range(TABLE_SIZE), k=K)\n",
    "    neg_list =[]\n",
    "    for i in rand:\n",
    "        print(unigram_table[i])\n",
    "        neg_list.append(unigram_table[i])\n",
    "    return neg_list\n",
    "\n",
    "\n",
    "probs = prob_frequency(count_set)\n",
    "su = subsampling(probs, filtered_sentence)\n",
    "neg_s = negative_sampling(su,alpha, TABLE_SIZE,K)\n",
    "neg_s  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "serial\nprevious\nsex\nseeking\nfloating\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n tensor([0., 0., 0.,  ..., 0., 0., 0.])]"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "def index_lookup(word, w_i = word_index):\n",
    "    return w_i[word]\n",
    "\n",
    "N = len(word_index)\n",
    "def train_preprocess(sentence,N,neg_list,CONTEXT_SIZE = CONTEXT_SIZE):\n",
    "    input_target_pair = []\n",
    "    neg_v = []\n",
    "    for  s in sentence:\n",
    "        tok_s = nt.word_tokenize(s)\n",
    "        for i, word in enumerate(tok_s):\n",
    "            for cont_ran in range(-CONTEXT_SIZE,CONTEXT_SIZE+1):\n",
    "                try:\n",
    "                    if cont_ran!=0 and i+cont_ran>=0:\n",
    "                        i_word = index_lookup(word)\n",
    "                        i_neri = index_lookup(tok_s[i+cont_ran])\n",
    "                        temp_w = to_categorical(i_word,N)\n",
    "                        temp_n = to_categorical(i_neri,N)\n",
    "                        temp = (torch.from_numpy(np.asarray(temp_w)), torch.from_numpy(np.asarray(temp_n)))\n",
    "                        #print(temp)\n",
    "                        input_target_pair.append(temp)   \n",
    "                except (IndexError or TypeError):\n",
    "                    continue\n",
    "    for n in neg_list:\n",
    "        print(n)\n",
    "        i_word = index_lookup(n)\n",
    "        temp_w = to_categorical(i_word,N)\n",
    "        t = torch.from_numpy(np.asarray(temp_w))\n",
    "        neg_v.append(t)\n",
    "    return input_target_pair, neg_v\n",
    "t,neg = train_preprocess(su,N,neg_s)\n",
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "        self.n_negs = n_negs\n",
    "        self.init_emb()\n",
    "        \n",
    "    def init_emb(self):\n",
    "        \"\"\"Initialize embedding weight like word2vec.\n",
    "            The u_embedding is a uniform distribution in [-0.5/em_size, 0.5/emb_size], and the elements of v_embedding are zeroes.\n",
    "            Returns:None\n",
    "        \"\"\"\n",
    "        initrange = 0.5 / self.emb_size\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "        #print(self.u_embeddings.weight.data.uniform_(-initrange, initrange))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SkipGram(\n  (u_embeddings): Embedding(5021, 50, sparse=True)\n  (v_embeddings): Embedding(5021, 50, sparse=True)\n)"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(t, batch_size=10, shuffle=True)\n",
    "SkipGram(N,EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}