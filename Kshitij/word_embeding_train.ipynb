{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "05c85ea36fc95bc4a1c36a3388b3862657f34df20b6eda5408346612bb633286"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk.tokenize as nt\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import random\n",
    "from torch.optim import SGD,Adam\n",
    "from torch.autograd import Variable\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocessing mobydick text to generate data and pass it to a skipgram model\n",
    "'''\n",
    "# this pasrt is only requied once to convert the downloaded text to a format that can be used for the model\n",
    "strings = \"\"\n",
    "with open(\"mobydick.txt\", encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        for l in re.split(r\"(\\. |\\? |\\! )\",line):\n",
    "            strings += l\n",
    "        strings += '\\n'\n",
    "s = strings.replace('\\n','')\n",
    "c = sent_tokenize(s)\n",
    "\n",
    "ff = open(\"mobydicktrim.txt\", \"w\", encoding='utf-8')\n",
    "for i in c:\n",
    "    #print(i)\n",
    "    if (len(i.split())>1):\n",
    "        s = re.sub(r'[^\\w\\s]',' ',i)\n",
    "        s = s.replace(\"_\",\"\")\n",
    "        ff.write(s.lower())\n",
    "        ff.write('\\n')\n",
    "ff.close()    "
   ]
  },
  {
   "source": [
    "# Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "thresh = float=1e-5\n",
    "TABLE_SIZE = 1e8\n",
    "alpha = 0.75\n",
    "CONTEXT_SIZE = 2 \n",
    "EMB_DIM = 50\n",
    "lr = 0.003\n",
    "batch_size = 512"
   ]
  },
  {
   "source": [
    "# Data Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done\nFound 26859 words in the dataset\nFound 26859 index in the dataset\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "200821"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "idx = 0 \n",
    "word_index = {}\n",
    "index_word = {}\n",
    "vocab = defaultdict(int)\n",
    "vocab_freq = {}\n",
    "total_words = 0\n",
    "min_count = 5\n",
    "sentence = []\n",
    "\n",
    "f = open('mobydicktrim.txt', encoding='utf-8')\n",
    "for i in f.readlines():\n",
    "    tok_sentence =  i.strip().split()\n",
    "    sentence.append(tok_sentence)\n",
    "    for word in tok_sentence:\n",
    "        vocab[word] += 1\n",
    "        total_words += 1\n",
    "# get word to index, index to word and frequncy of the words\n",
    "for k,v in vocab.items():\n",
    "    vocab_freq[k] = v\n",
    "    word_index[k]=idx\n",
    "    index_word[idx] = k\n",
    "    idx+=1\n",
    "print(\"Done\")\n",
    "print(\"Found %d words in the dataset\" %len(word_index))\n",
    "print(\"Found %d index in the dataset\" %len(index_word))\n",
    "\n",
    "N = len(vocab_freq)\n",
    "total_words"
   ]
  },
  {
   "source": [
    "# Sub-Sampling the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Removing frequent occuring words \n",
    "'''\n",
    "def sub_sampling(freq, thresh = thresh):\n",
    "    return 1 - ((thresh/freq)**0.5 + (thresh/freq))\n",
    "\n",
    "freq = {k : v/total_words for k,v in vocab_freq.items()}\n",
    "freq = {k : sub_sampling(v) for k,v in freq.items()}\n",
    "train = []\n",
    "c=0\n",
    "for s in sentence:\n",
    "    temp = []\n",
    "    for i in s:\n",
    "        if (random.random() < v):\n",
    "            temp.append(i)\n",
    "            c+=1\n",
    "    train.append(temp)"
   ]
  },
  {
   "source": [
    "# Negative Subsamples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[62208.  3039.  1333. ...  1333.  1333.  1333.]\n"
    }
   ],
   "source": [
    "def negs(freq,alpha = alpha, TABLE_SIZE=TABLE_SIZE):\n",
    "    '''\n",
    "    Calculates the probaility of the negative subsampled words and generates a unigram table based on these probs \n",
    "    '''\n",
    "    unigram_table = []\n",
    "    pow_freq = np.array(list(freq.values())) ** alpha\n",
    "    tot_pow = sum(pow_freq)\n",
    "    r = pow_freq / tot_pow\n",
    "    count = np.round(r * TABLE_SIZE)\n",
    "    print(count)\n",
    "    for item_id, _count in enumerate(count):\n",
    "        unigram_table += [item_id] * int(_count)\n",
    "    unigram_table = np.array(unigram_table)\n",
    "    return unigram_table\n",
    "\n",
    "def get_neg_samples(unigram_table, k=5, TABLE_SIZE=TABLE_SIZE):\n",
    "    '''\n",
    "    fetches random negative samples from unigram table\n",
    "    '''\n",
    "    rand = random.choices(range(TABLE_SIZE), k=k)\n",
    "    neg_list =[]\n",
    "    for i in rand:\n",
    "        neg_list.append(unigram_table[i])\n",
    "    return neg_list\n",
    "n = negs(vocab_freq)"
   ]
  },
  {
   "source": [
    "# Create training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(0, 1, array([9518, 2280,   39,  684,  905], dtype=int64)),\n (1, 0, array([1905, 4666, 2122,   39,   44], dtype=int64)),\n (2, 3, array([ 2001, 24685,   159, 13723, 26317], dtype=int64)),\n (2, 4, array([ 8654, 16336,    98,  3591, 10321], dtype=int64)),\n (3, 2, array([ 8033,  1639, 19284,  3673,  2001], dtype=int64)),\n (3, 4, array([2191,  455, 1273, 7853,  243], dtype=int64)),\n (3, 5, array([ 3158,   159,    39,    34, 10938], dtype=int64)),\n (4, 2, array([   19,    41,   359, 14882, 18089], dtype=int64)),\n (4, 3, array([11204, 10744, 20171,   138,  2167], dtype=int64)),\n (4, 5, array([26818, 21116,  3689, 16172,  8274], dtype=int64))]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "def index_lookup(word, w_i = word_index):\n",
    "    '''\n",
    "    returns index based on word \n",
    "    '''\n",
    "    return w_i[word]\n",
    "\n",
    "def word_lookup(idx, i_w = index_word):\n",
    "    '''\n",
    "     returns the word based on index\n",
    "    '''\n",
    "    return i_w[idx]\n",
    "\n",
    "def train_preprocess(sentence,unigram_table,K,TABLE_SIZE=int(TABLE_SIZE),CONTEXT_SIZE = CONTEXT_SIZE):\n",
    "    '''\n",
    "        generates word and target pairs with 5 negative subsampled values which is fed to the model \n",
    "    '''\n",
    "    input_target_pair = []\n",
    "    neg_v = []\n",
    "    for  s in sentence:\n",
    "        for i, word in enumerate(s):\n",
    "            for cont_ran in range(-CONTEXT_SIZE,CONTEXT_SIZE+1):\n",
    "                try:\n",
    "                    if cont_ran!=0 and i+cont_ran>=0:\n",
    "                        i_word = index_lookup(word)\n",
    "                        i_target = index_lookup(s[i+cont_ran])\n",
    "                        negs = get_neg_samples(unigram_table, K, TABLE_SIZE)\n",
    "                        temp = (i_word, i_target, np.array(negs, dtype=np.int64))\n",
    "                        input_target_pair.append(temp)   \n",
    "                except (IndexError or TypeError):\n",
    "                    continue\n",
    "    return input_target_pair\n",
    "tr = train_preprocess(train, n, 5)\n",
    "tr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loads the preprocessed training data into batches \n",
    "trainloader = torch.utils.data.DataLoader(tr, batch_size=batch_size)"
   ]
  },
  {
   "source": [
    "# Skip-Gram model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, emb_size,sparse=True)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, emb_size,sparse=True)\n",
    "        # initalize wmbedding weights\n",
    "        initrange = 0.5 / self.emb_size\n",
    "        self.in_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embeddings.weight.data.uniform_(0,0)\n",
    "        \n",
    "    def forward(self,word,target,negs):\n",
    "        u = self.in_embeddings(word)\n",
    "        v = self.out_embeddings(target)\n",
    "        pos_vals = torch.mul(u,v).squeeze()\n",
    "        pos_vals = torch.sum(pos_vals, dim=1)\n",
    "        pos_vals = F.logsigmoid(pos_vals)\n",
    "        v_hat = self.out_embeddings(negs)               \n",
    "        neg_vals =  torch.bmm(v_hat, u.unsqueeze(2)).squeeze()\n",
    "        neg_vals = F.logsigmoid(-neg_vals)\n",
    "        l = -(torch.sum(pos_vals) + torch.sum(neg_vals))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
    "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
    "        Here, embedding should be a PyTorch embedding module.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we're calculating the cosine similarity between some random words and \n",
    "    # our embedding vectors. With the similarities, we can look at what words are\n",
    "    # close to our random words.\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
    "        \n",
    "    return valid_examples, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SkipGram(N,EMB_DIM)\n",
    "optimizer = SGD(net.parameters(), lr=lr)\n",
    "net = net.cuda()"
   ]
  },
  {
   "source": [
    "# Train the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1,  1400] loss: 4.127\nwarehouses | pressure, heigh, whereonthey, uncomfortableinfliction, insure\nupper | iron, way, ve, even, world\nespecially | large, play, might, oil, sort\nwatery | next, involving, pictorial, used, heat\nthat | he, as, so, but, s\nwould | have, it, are, i, not\nbefore | as, s, but, like, and\nstrong | drops, main, iclaim, november, memorable\nweary | crew, stood, aloft, sound, standing\nbrought | nor, off, leviathan, harpoon, last\nwindows | higherthan, mingled, harbourless, intervals, rapacious\nofa | stress, monstrouscabinet, swordbetween, transferring, fair\npieces | beat, skeleton, waters, view, heart\nwisdom | depth, breathest, gardenny, thedantean, aresome\nboots | pipe, frombuffalo, thickens, mindfor, avalor\ninches | intervening, weare, witharsacidean, hearses, leaping\n...\n\n[2,  1400] loss: 3.791\nyears | then, can, or, stubb, who\nit | some, this, their, him, an\nknocking | readily, dumb, vital, drawn, thedeck\nandmethodically | wideintervals, ofqueens, symphony, arepecking, asylum\nparticular | this, some, it, those, their\npart | in, sea, whale, white, great\ns | which, other, as, more, or\ndamp | base, east, rock, overboard, magnitude\ntoinquire | issue, bore, due, hailed, minutes\nconclude | dawn, honor, flying, closely, latitudes\ndubious | cunning, outpincers, willed, perceived, tellest\navery | damn, asia, recollection, doublyhammered, locked\nsteps | list, ahabthat, pretty, leaning, burning\ngloom | helm, hundred, case, fish, english\ncomparing | drag, furnished, beams, straightened, drink\nplight | officers, cask, speaking, help, lad\n...\n\n[3,  1400] loss: 3.472\nrequires | rocks, figure, slightest, pulpit, grave\nhand | over, round, with, long, after\nsuch | had, only, though, without, made\nrear | deck, air, boats, from, body\nit | some, him, an, this, their\nishmael | passed, ere, sailor, spout, why\ns | out, sperm, whales, full, sort\ndeliberately | swift, considering, height, lightning, crown\ndestined | cold, fight, help, tashtego, escaped\nsure | ever, we, flask, aye, been\ndiscover | objects, flesh, partly, mountains, need\ntinkling | shouldconceive, shrunk, feetout, celebratedtribe, snarls\ndark | down, where, men, water, man\nsoles | proudly, excellently, aseal, shipindolently, shiveredlance\ncongealed | hadsounded, flowered, arid, shrieks, thetown\nport | monster, on, deck, first, present\n...\n\n[4,  1400] loss: 3.283\nwatery | next, mate, world, instant, crew\ninterest | face, upon, bone, till, while\nhats | ropes, foul, buried, thepresent, monkey\ndamp | base, rock, magnitude, colour, peculiar\nwarehouses | uncomfortableinfliction, whereonthey, insure, syren, pressure\ninvoluntarily | floated, hinted, read, wonder, ago\npurse | weapons, voyages, streets, dropping, grey\npart | same, line, pequod, body, bottom\ndreary | wants, cries, accomplished, confidential, entering\nmeanwhile | stood, rather, often, years, whose\nharpoons | two, three, both, or, ships\ncongealed | shrieks, thetown, refuge, arising, examine\nsign | heart, dead, night, length, water\nfromhard | thoseicy, allswooping, thechains, thissperm, miguel\nbroad | than, off, after, little, eyes\nthick | work, way, dark, table, hold\n...\n\n[5,  1400] loss: 3.163\nishmael | call, why, unless, give, shipmates\n1 | plumage, cetacea, dipped, rum, monsters\nshore | sides, over, round, while, night\ncall | does, why, give, saw, unless\nit | he, that, him, but, so\npurse | voyages, weapons, streets, dropping, key\ndriving | turning, up, merchant, least, came\nis | was, as, but, were, so\nere | passed, comes, certain, along, beneath\ncoldand | quickest, romishfaith, feats, repugnance, passengers\nday | right, round, most, long, mast\nsoles | proudly, andin, englander, sillof, mindedness\nmostmiserable | twigged, thoughtthat, medicament, gregarious, outbellying\nfrost | struck, calm, filled, leading, phantom\nprojections | eats, layers, ferry, restricted, puffs\nbroad | than, off, vast, eyes, arm\n...\n\n[6,  1400] loss: 3.076\non | from, by, at, with, into\nwhenevermy | whichever, ofold, anyhow, withal, understood\nup | down, away, board, god, standing\naccount | back, without, off, curious, light\ngrim | near, making, arms, together, behind\nit | he, there, what, ahab, but\ndrizzly | horizontalposition, bells, revery, distrustful, famousold\ncall | does, give, saw, why, unless\nofa | lad, gentlemen, ears, else, ago\nlodge | begged, lady, interesting, foremast, parted\nanxious | reach, ill, forthe, sleep, st\npacked | colossal, blow, noise, also, several\nthebowsprit | an, my, some, nervousness, their\nharpoon | lord, foot, hole, interval, instant\ndiscover | dim, dinner, partly, mountains, spot\nsaid | thought, did, well, if, will\n...\n\n[7,  1400] loss: 3.009\ndamp | suspended, black, oceans, day, chief\nwhenever | call, read, pretty, regard, show\nnothing | always, she, bildad, once, anything\ninvoluntarily | hinted, felt, feel, external, canst\nwould | may, must, will, can, could\nstepping | ground, higher, beef, mariners, gods\nshore | sides, shot, cutting, ran, chief\nwhenevermy | whichever, ofold, anyhow, withal, array\ntoinquire | mariner, glistening, lashing, delivery, strain\nflinty | tempestuous, fobbing, begrimed, extra, oughts\npocket | pipe, craft, under, grand, green\ndreary | start, empty, tothe, years, downwards\navery | possess, tranquilly, experiences, argued, thatsame\namatter | angels, rises, aship, striking, setting\nthenorth | 15th, manthat, hen, adauntless, whirled\nbitingly | congealed, handspike, sandwich, accosted, resume\n...\n\n[8,  1400] loss: 2.953\n1 | plumage, smiting, addressed, monsters, diving\nhats | straight, jaws, heads, lines, force\nget | turn, thee, help, enough, st\nof | from, in, on, winding, with\nthat | but, which, as, one, whale\nespecially | wise, slowly, caught, tashtego, further\nnothing | always, she, once, flask, bildad\nway | strange, night, bed, place, first\nbright | heavy, wide, people, spring, morning\ncheerless | thetwo, bowed, sister, firmly, accounted\nwherever | supper, die, since, brute, wonder\ndarkness | hot, dim, bones, fifty, o\nthebowsprit | atthis, my, holloa, an, downthe\ninches | dutch, thousand, grave, warm, virtue\nrays | suddenly, lie, avast, several, went\nlodge | transported, manxman, ear, wound, lakeman\n...\n\n[9,  1400] loss: 2.904\nwhenever | read, call, spoke, perceived, needs\nand | on, from, were, over, into\nthen | yet, now, here, again, stubb\ninto | upon, from, on, over, with\nandbringing | eddy, awaiting, antiquity, suspicions, systematically\nworld | crew, body, matter, instant, end\nto | for, at, pours, but, over\ndrizzly | bells, club, felicity, philosopher, derick\navery | experiences, thatsame, argued, therope, tranquilly\nwindows | weather, owners, parts, intervals, bows\ndismal | laugh, blanket, distinct, terrible, thousand\nfervent | beganto, expensive, thepistol, lazarus, wantest\nlodge | transported, manxman, ear, screwed, wound\ndubious | tells, stately, pillow, affair, grey\nmostmiserable | gregarious, burnround, outbellying, idolator, phrenologists\nwisdom | lady, needles, mariner, wooden, cannibal\n...\n\n[10,  1400] loss: 2.861\nwhenever | read, spoke, call, felt, answered\nrequires | hoops, stage, becomes, terrible, thousand\nnothing | flask, indeed, she, done, once\nandmethodically | chaseagainst, moluccas, bank, gazer, thoughtful\nespecially | wise, slowly, caught, suddenly, tashtego\n1 | plumage, smiting, highest, standers, original\nprecisely | dish, mark, nevertheless, ere, really\nfrom | on, into, upon, with, by\ncoldand | agencies, ding, endurance, sprained, quickest\nsign | mere, law, terror, thick, honor\nrisk | inner, aspects, final, canvas, ultimate\nhear | find, oh, art, believe, die\nprojections | bury, river, eats, doesn, hasn\npassed | entirely, hail, hence, advancing, besides\nstruck | true, carpenter, itself, best, caught\ninches | dutch, sitting, grave, thousand, ice\n...\n\nFinished Training\n"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    avg_loss = 0\n",
    "    j = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels, negs = data\n",
    "        inputs = Variable(torch.LongTensor(inputs))\n",
    "        labels = Variable(torch.LongTensor(labels))\n",
    "        negs = Variable(torch.LongTensor(negs))\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        negs = negs.cuda()\n",
    "        loss = net(inputs,labels,negs)\n",
    "        # zero the parameter gradients \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "        # print statistics\n",
    "        if i % 1400 == 1399:    # print every 1400 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, avg_loss/(1399*batch_size)))\n",
    "            valid_examples, valid_similarities = cosine_similarity(net.in_embeddings, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6)\n",
    "            avg_loss = 0.0\n",
    "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [word_lookup(idx.item()) for idx in closest_idxs[ii]][1:]\n",
    "                print(word_lookup(valid_idx.item()) + \" | \" + ', '.join(closest_words))\n",
    "            print(\"...\\n\")\n",
    "print('Finished Training')"
   ]
  }
 ]
}